{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "591b50c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import dependencies\n",
    "import tensorflow as tf\n",
    "import random \n",
    "import gym\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation, Flatten, Conv2D, MaxPooling2D\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import gym_super_mario_bros\n",
    "from gym_super_mario_bros.actions import RIGHT_ONLY\n",
    "from nes_py.wrappers import JoypadSpace\n",
    "from IPython.display import clear_output\n",
    "\n",
    "from tensorflow.keras.models import save_model\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0d250999",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.6.2'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ac8c8f34",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create our environment\n",
    "env=gym_super_mario_bros.make(\"SuperMarioBros-v0\")\n",
    "env=JoypadSpace(env, RIGHT_ONLY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "82683bf2",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_32584/3114917832.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m         \u001b[0mstate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m     \u001b[0mstate\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minfo\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maction_space\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msample\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minfo\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[0mtotal_reward\u001b[0m\u001b[1;33m+=\u001b[0m\u001b[0mreward\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\chirag\\anaconda3\\envs\\my_rl_env\\lib\\site-packages\\nes_py\\wrappers\\joypad_space.py\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     72\u001b[0m         \"\"\"\n\u001b[0;32m     73\u001b[0m         \u001b[1;31m# take the step and record the output\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 74\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_action_map\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     75\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     76\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mreset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\chirag\\anaconda3\\envs\\my_rl_env\\lib\\site-packages\\gym\\wrappers\\time_limit.py\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m         \u001b[1;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_elapsed_steps\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"Cannot call env.step() before calling reset()\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m         \u001b[0mobservation\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_elapsed_steps\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_elapsed_steps\u001b[0m \u001b[1;33m>=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_max_episode_steps\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\chirag\\anaconda3\\envs\\my_rl_env\\lib\\site-packages\\nes_py\\nes_env.py\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m    291\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontrollers\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0maction\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    292\u001b[0m         \u001b[1;31m# pass the action to the emulator as an unsigned byte\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 293\u001b[1;33m         \u001b[0m_LIB\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mStep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_env\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    294\u001b[0m         \u001b[1;31m# get the reward for this step\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    295\u001b[0m         \u001b[0mreward\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_reward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# we are rendering a random agent here..\n",
    "total_reward=0\n",
    "done=True\n",
    "\n",
    "for step in range(100000):\n",
    "    env.render()\n",
    "    if done:\n",
    "        state=env.reset()\n",
    "    state,reward,done, info=env.step(env.action_space.sample())\n",
    "    print(info)\n",
    "    total_reward+=reward\n",
    "    clear_output(wait=True)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e203a4df",
   "metadata": {},
   "outputs": [],
   "source": [
    "#class DQN Agent.. brain of the agent\n",
    "\n",
    "class DQNAgent:\n",
    "    def __init__(self, state_size, action_size):\n",
    "        #create variables for our agent\n",
    "        self.state_size = state_size\n",
    "        self.action_space = action_size\n",
    "        self.memory = deque(maxlen=5000)\n",
    "        \n",
    "        #exploration vs exploitation\n",
    "        self.epsilon = 1\n",
    "        self.max_epsilon = 1\n",
    "        self.main_epsilon = 0.01\n",
    "        self.decay_epsilon = 0.0001\n",
    "        \n",
    "        #building a Neural network for agent\n",
    "        self.main_network = self.build_network()\n",
    "        self.target_network = self.build_network()\n",
    "        \n",
    "        self.update_target_network()\n",
    "        \n",
    "    def build_network(self):\n",
    "        model = Sequential()\n",
    "        #add convolution layer\n",
    "        model.add(Conv2D(64, (4,4), strides = 4, padding ='same', input_shape=self.state_shape))\n",
    "        model.add(Activation('relu'))\n",
    "        \n",
    "        model.add(Conv2D(64,(4,4),strides=2, padding = 'same'))\n",
    "        model.add(Activation('relu'))\n",
    "        \n",
    "        model.add(Conv2D(64,(3,3),strides=2, padding ='same'))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(Flatten())\n",
    "        \n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(512),activation='relu')\n",
    "        model.add(Dense(256),activation='relu')\n",
    "        model.add(Dense(128),activation='relu')\n",
    "        model.add(Dense(self.action_space, activation='linear'))\n",
    "        \n",
    "        model.compile(loss='mse', optimizer=Adam())\n",
    "        \n",
    "        \n",
    "    def update_target_networ(self):\n",
    "        self.target_network.set_weights(self.main_network.get_weights())\n",
    "        \n",
    "    def act(self, state):\n",
    "        #if epsilon is still large than we want to take a random action\n",
    "        if random.uniform(0,1)<self.epsilon:\n",
    "            return np.random_randint(self.action_space)\n",
    "        Q_value=self.main_network.predict(state)\n",
    "        \n",
    "        return np.argmax(Q_value[0])\n",
    "    \n",
    "    #researchers have found this epsilon learning procedure to be most fruit full\n",
    "    \n",
    "    def update_epsilon(self, episode):\n",
    "        \n",
    "        self.epsilon= self.min_epsilon + (self.max_epsilon-self.min_epsilon)*np.exp(-self.epsilon_decay*episode)\n",
    "    \n",
    "    def train(self, batch_size):\n",
    "        #minibatch for memory\n",
    "        minibatch = random.sample(self.memory, batch_size)\n",
    "        \n",
    "        #Get variables from batch so we can find q-value\n",
    "        \n",
    "        for state, action, reward, next_state, done in minibatch:\n",
    "            target = self.main_network.predict(state)\n",
    "            \n",
    "            if done:\n",
    "                target[0][action] = reward\n",
    "            else:\n",
    "                target[0][action] = (reward+self.gamma*np.amax(self.target_network.predict(next_state)))\n",
    "            \n",
    "            self.main_network.fit(state, target, epochs=1, verbose =0)\n",
    "    def store_transition(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "054f202c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Box(0, 255, (240, 256, 3), uint8)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "action_space=env.action_space.n\n",
    "state_size=(80,88,1)\n",
    "from PIL import IMAGE\n",
    "def preprocess_state(state):\n",
    "    image = Image.fromarray(state)\n",
    "    image=image.resize(88,80)\n",
    "    image = image.convert('L')\n",
    "    image=np.array(image)\n",
    "    \n",
    "    return image\n",
    "env.observation_space\n",
    "#dqn=DQNAgent(state_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a1e2f4d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
